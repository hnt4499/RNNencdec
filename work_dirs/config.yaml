data:
    bitext_files:
        train:
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.0-15
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.15-20
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.20-25
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.25-30
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.30-35
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.35-40
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.40-45
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.45-55
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.55-75
            - /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/training_data.en-fr.75-175
        val: /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/val_data.en-fr
        test: /media/jupyter/Data/dataset/NLP/text/MT/processed_3rd_step/test_data.en-fr
    num_samples:  # number of samples in each training files
        - 8388427
        - 5020862
        - 4968025
        - 4327915
        - 3457298
        - 2618308
        - 1897687
        - 2318231
        - 1818877
        - 973643
    src_vocab_path: /home/jupyter/Documents/projects/seq2seq/data/vocab.30000.en-fr.en
    tgt_vocab_path: /home/jupyter/Documents/projects/seq2seq/data/vocab.30000.en-fr.fr

model:
    embedding_dim: 512
    encoder_hidden_dim: 512
    decoder_hidden_dim: 512
    attention_dim: 256
    bidirectional: true
    num_layers: 1
training:
    # Set to `null` to not save anything
    work_dir: /home/jupyter/Documents/projects/seq2seq/work_dirs/
    dropout: 0.0
    device: cuda
    learning_rate: 0.01
    # Global batch size if int, or can be a list of batch sizes corresponding
    # to each training file
    batch_size:
        - 256
        - 64
        - 64
        - 64
        - 48
        - 48
        - 48
        - 32
        - 32
        - 8
    num_epochs: 1000
    num_workers: 1
    gradient_clip: 1  # set to None to ignore gradient clipping
    # Set to 'true' to allow 10 iterations per epoch
    testing: false
evaluating:
    evaluate_every: 1000  # iteration interval
    evaluate_bleu: true
    bleu_config:
        print_samples: true
        nbest: 1  # print `n` best hypotheses for each source sentence
        nprint: 5  # number of samples to print
        tgt_lang: fr
        unescape: false

        beam_size: 5
        max_len_a: 0
        max_len_b: 200
        min_len: 1
        normalize_scores: True
        len_penalty: 1.0
        unk_penalty: 0.0
        temperature: 1.0
